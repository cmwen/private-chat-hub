# Quick Reference: Local & Remote Model System

**Version:** 1.0 | **Date:** January 25, 2026

---

## üéØ At a Glance

**What:** Unified system for local (LiteRT) and remote (Ollama) AI models with automatic offline support

**Key Features:**
- ‚úÖ One model selector for all models
- ‚úÖ Automatic routing (local vs remote)
- ‚úÖ Offline message queueing
- ‚úÖ Local model fallback
- ‚úÖ Real-time connection monitoring

---

## üìÅ Key Files

| File | Purpose | Lines |
|------|---------|-------|
| [unified_model_service.dart](../lib/services/unified_model_service.dart) | Combine local + remote models | ~100 |
| [connectivity_service.dart](../lib/services/connectivity_service.dart) | Monitor Ollama connection | ~150 |
| [message_queue_service.dart](../lib/services/message_queue_service.dart) | Queue offline messages | ~250 |
| [chat_service.dart](../lib/services/chat_service.dart) | Router & coordinator | ~2100 |

---

## üèóÔ∏è Architecture

```
User ‚Üí ChatService ‚Üí { Local? ‚Üí LiteRT
                     { Remote + Online? ‚Üí Ollama
                     { Remote + Offline? ‚Üí Queue
```

---

## üîß Core APIs

### UnifiedModelService

```dart
// Get all models (local + remote)
Future<List<ModelInfo>> getUnifiedModelList(
  List<OllamaModelInfo> ollamaModels
)

// Check model type
static bool isLocalModel(String modelId)
static bool isRemoteModel(String modelId)

// Local models have 'local:' prefix
// Example: 'local:gemma3-1b', 'llama3:latest'
```

### ChatService

```dart
// Send message (auto-routes)
Stream<Conversation> sendMessage(String conversationId, String text)

// Queue message (offline)
Future<Conversation> queueMessage(String conversationId, String text)

// Process queue (when online)
Future<void> processMessageQueue()

// Connection status
bool get isOnline
bool get isOffline
```

### MessageQueueService

```dart
// Add to queue
Future<void> enqueue({
  required String conversationId,
  required String messageId,
})

// Get next item (FIFO)
QueueItem? getNextQueueItem()

// Remove from queue
Future<void> remove(String itemId)

// Queue status
int getQueueCount()
Stream<List<QueueItem>> get queueUpdates
```

### ConnectivityService

```dart
// Status stream
Stream<OllamaConnectivityStatus> get statusStream

// Current status
OllamaConnectivityStatus get currentStatus

// Quick checks
bool get isOnline
bool get isOffline

// Manual refresh
Future<void> refresh()
```

---

## üé® Model Identification

### Local Models

```dart
// Format: 'local:' + modelId
'local:gemma3-1b'
'local:gemma-3n-e2b'
'local:phi-4-mini'

// Detection
UnifiedModelService.isLocalModel('local:gemma3-1b') // true

// Icon: üì±
// Label: "On-Device"
```

### Remote Models

```dart
// Format: Ollama model name
'llama3:latest'
'mistral:latest'
'codellama:7b'

// Detection
UnifiedModelService.isRemoteModel('llama3:latest') // true

// Icon: üåê
// Label: None (standard)
```

---

## üìä Message Status

### Status Enum

```dart
enum MessageStatus {
  draft,    // Being composed
  queued,   // Waiting to send (offline)
  sending,  // Currently sending
  sent,     // Successfully sent
  failed,   // Send failed
}
```

### Status Icons

| Status | Icon | Color | Meaning |
|--------|------|-------|---------|
| sent | ‚úì | Green | Delivered successfully |
| queued | üì§‚è≥ | Orange | Will send when online |
| sending | ‚åõ | Blue | Transmitting now |
| failed | ‚ö†Ô∏è | Red | Failed (can retry) |

---

## üîÑ Routing Logic

```dart
// Pseudocode
if (model.startsWith('local:')) {
  ‚Üí LiteRT
} else if (isOnline) {
  ‚Üí Ollama
} else if (hasLocalModel) {
  ‚Üí Prompt fallback or Queue
} else {
  ‚Üí Queue
}
```

---

## üíæ Storage Schema

### Message

```dart
{
  "id": "uuid",
  "role": "user|assistant",
  "text": "string",
  "timestamp": "ISO8601",
  "status": "draft|queued|sending|sent|failed",  // NEW
  "queuedAt": "ISO8601?"  // NEW
}
```

### Queue Item

```dart
{
  "id": "uuid",
  "conversationId": "uuid",
  "messageId": "uuid",
  "enqueuedAt": "ISO8601",
  "retryCount": 0-3,
  "lastRetryAt": "ISO8601?"
}
```

---

## üéÆ Usage Examples

### Select Model (Unified)

```dart
// Get all models
final unifiedService = UnifiedModelService(
  onDeviceLLMService: onDeviceService,
);

final ollamaModels = await ollamaManager.listModels();
final allModels = await unifiedService.getUnifiedModelList(ollamaModels);

// Show to user
for (final model in allModels) {
  print('${model.isLocal ? 'üì±' : 'üåê'} ${model.name}');
}
```

### Send Message (Auto-Route)

```dart
// ChatService handles routing automatically
final conversationStream = await chatService.sendMessage(
  conversationId,
  'Hello!',
);

// Subscribe to updates
conversationStream.listen((conversation) {
  // UI updates with latest conversation state
  setState(() => _conversation = conversation);
});
```

### Monitor Connection

```dart
// Listen to connectivity changes
chatService.connectivityService.statusStream.listen((status) {
  switch (status) {
    case OllamaConnectivityStatus.connected:
      print('‚úÖ Online');
      break;
    case OllamaConnectivityStatus.disconnected:
      print('‚ö†Ô∏è Ollama offline');
      break;
    case OllamaConnectivityStatus.offline:
      print('üîå No network');
      break;
  }
});
```

### Process Queue

```dart
// Automatic (when connection restored)
chatService.connectivityService.statusStream.listen((status) {
  if (status == OllamaConnectivityStatus.connected) {
    chatService.processMessageQueue(); // Auto-called
  }
});

// Manual trigger
await chatService.processMessageQueue();

// Get queue count
final count = chatService.queueService.getQueueCount();
print('$count messages queued');
```

---

## üéØ Common Patterns

### Check Model Type

```dart
final modelId = conversation.modelName;

if (UnifiedModelService.isLocalModel(modelId)) {
  // Use LiteRT
  print('Using local model');
} else {
  // Use Ollama
  print('Using remote model');
}
```

### Handle Offline

```dart
if (!chatService.isOnline) {
  // Show offline indicator
  showBanner('Offline - messages will queue');
}

// Messages still send (will queue)
await chatService.sendMessage(conversationId, text);
```

### Retry Failed Message

```dart
final message = conversation.messages
    .firstWhere((m) => m.status == MessageStatus.failed);

// Option 1: Re-queue
await chatService.queueService.enqueue(
  conversationId: conversationId,
  messageId: message.id,
);

// Option 2: Resend immediately (if online)
await chatService.retryMessage(conversationId, message.id);
```

### Local Fallback

```dart
if (!chatService.isOnline && 
    chatService.onDeviceLLMService != null &&
    await chatService.isOnDeviceAvailable()) {
  
  // Offer fallback
  showDialog(
    'Ollama offline. Use local model instead?',
    onConfirm: () => switchToLocalModel(),
  );
}
```

---

## ‚öôÔ∏è Configuration

### Inference Mode

```dart
enum InferenceMode {
  remote,    // Always use Ollama
  onDevice,  // Always use LiteRT
}

// Set mode
await inferenceConfigService.setInferenceMode(InferenceMode.onDevice);

// Get current mode
final mode = chatService.currentInferenceMode;
```

### Queue Settings

```dart
// Max queue size
MessageQueueService._maxQueueSize = 50;

// Retry delays (in seconds)
MessageQueueService._retryDelays = [0, 5, 15];
```

### Connection Monitoring

```dart
// Health check interval
ConnectivityService._healthCheckInterval = Duration(seconds: 30);

// Manual check
await chatService.connectivityService.refresh();
```

---

## üêõ Error Handling

### Queue Full

```dart
try {
  await queueService.enqueue(
    conversationId: id,
    messageId: msgId,
  );
} on Exception catch (e) {
  if (e.toString().contains('Queue is full')) {
    showError('Queue full. Please wait or clear queue.');
  }
}
```

### Model Not Available

```dart
try {
  await onDeviceService.loadModel(modelId);
} on Exception catch (e) {
  showDialog(
    'Model not available. Download now?',
    onConfirm: () => downloadModel(modelId),
  );
}
```

### Connection Timeout

```dart
try {
  await ollamaManager.sendMessage(...);
} on TimeoutException {
  // Auto-queued by ChatService
  // Show offline indicator
}
```

---

## üìà Performance

### Benchmarks

| Operation | Time |
|-----------|------|
| Model type check | < 1ms |
| Queue enqueue | < 10ms |
| Queue dequeue | < 5ms |
| Connection check | 50-200ms |
| Local inference (first token) | 50-200ms |
| Remote inference (first token) | 100-500ms |

### Memory Usage

| Component | Memory |
|-----------|--------|
| Queue service | < 1 MB |
| Connectivity service | < 500 KB |
| Unified model service | < 2 MB |
| Message (each) | ~1 KB |
| Queue item (each) | ~500 bytes |

---

## üß™ Testing

### Unit Test Example

```dart
test('Routes to local model correctly', () async {
  final chatService = ChatService(...);
  final conversation = Conversation(
    modelName: 'local:gemma3-1b',
    ...
  );
  
  final stream = chatService.sendMessage(conversation.id, 'Hi');
  
  // Verify uses LiteRT
  verify(mockOnDeviceService.generateResponse(...)).called(1);
  verifyNever(mockOllamaManager.sendMessage(...));
});
```

### Integration Test Example

```dart
testWidgets('Queue processes when online', (tester) async {
  // Go offline
  await connectivityService.setOffline();
  
  // Send message (should queue)
  await chatService.sendMessage(conversationId, 'Test');
  expect(queueService.getQueueCount(), 1);
  
  // Go online
  await connectivityService.setOnline();
  await tester.pumpAndSettle();
  
  // Verify queue processed
  expect(queueService.getQueueCount(), 0);
});
```

---

## üîó Related Docs

**Complete Documentation:**
- [üìã Documentation Index](LOCAL_REMOTE_MODEL_SYSTEM_INDEX.md)
- [üé® UX Design](UX_DESIGN_LOCAL_REMOTE_MODEL_SYSTEM.md)
- [üèóÔ∏è Architecture](ARCHITECTURE_LOCAL_REMOTE_MODEL_SYSTEM.md)
- [üìñ User Guide](USER_GUIDE_LOCAL_REMOTE_MODELS.md)
- [‚úÖ Implementation Summary](IMPLEMENTATION_SUMMARY_LOCAL_REMOTE_SYSTEM.md)

**Existing LiteRT Docs:**
- [LiteRT Integration Audit](../audit/LITERT_INTEGRATION_AUDIT.md)
- [LiteRT Quick Reference](../audit/LITERT_QUICK_REFERENCE.md)
- [LiteRT Implementation Guide](../audit/LITERT_IMPLEMENTATION_GUIDE.md)

---

## üéì Key Takeaways

1. **One unified list** - Users see all models together
2. **Auto-routing** - System chooses correct backend
3. **Offline transparent** - Queue handles it automatically
4. **Local = offline** - Local models work anywhere
5. **FIFO queue** - Messages send in order

---

## üí° Pro Tips

**For Developers:**
- Always check model type before routing
- Listen to connectivity stream for status updates
- Use queue service for offline handling
- Test with mock services for reliability

**For Users:**
- Download local models for offline use
- Use local models for privacy
- Let queue process automatically
- Check connection status before important messages

---

**Quick Links:**
- GitHub: [Issues](https://github.com/yourusername/private-chat-hub/issues)
- Docs: [Full Documentation](LOCAL_REMOTE_MODEL_SYSTEM_INDEX.md)
- Support: [User Guide](USER_GUIDE_LOCAL_REMOTE_MODELS.md)

---

**Last Updated:** January 25, 2026  
**Version:** 1.0  
**Status:** Core Complete, UI Integration Pending
